---
creation date: 2021-09-23 16:03
---
#  Grid Computing Lecture 2
23rd September 2021

## MPI
Message Passing Interface
```
mpirun (Open MPI) 4.1.1

Usage: mpirun [OPTION]...  [PROGRAM]...
Start the given program using Open RTE

-c|-np|--np <arg0>       Number of processes to run
-h|--help <arg0>         This help message
   -n|--n <arg0>         Number of processes to run
-q|--quiet               Suppress helpful messages
-v|--verbose             Be verbose
-V|--version             Print version and exit
```
- Runtime.
- https://stackoverflow.com/questions/18243379/difference-b-w-mpi-tcp-ip/18243600
- Standard for passing messages between processes.
- [[Grid Computing/1#SPMD]].
- **Distributed**: Memory on unique to each processor.
- **Shared**: Memory shared between processors.
- **Hybrid**: Mixture of both.

## OpenMPI C Library
- `MPI_X` prefix to functions.
- Return code format: `MPI_SUCCESS` const.
- `MPI_Init(&argc, &argv)` called at start of program.
	 - Arguments come from `mpirun`. (Num processed to run, etc)
- `MPI_COMM_WORLD`: Parallel context. Idenitifies the global communication context.
	- [[#Communicators]]
### Elements
- Size: number of processes. `MPI_Comm_size(MPI_COMM_WORLD, &size)` (sets size from global context).
- Rank: Identifies each processor.
	- Acquire data using rank ID.
- Processor name `MPI_Get_processor_name`
	- Unique to actual node.
	- From hardware.
- World Time `MPI_Wtime()`.
	- Time from beginning of global execution.
	
### Communicators
- Can only communicate within the communicator.
- Each process has a rank within, not outside, acts as ID.

## Block Partition
### Basic Strategy
- Partition at `(rank+1)*(n/size-1)`.
- Rank is incremental based on processor and starts at 0.
- Slower as rank gets higher due to dependency on previous data.
- Code gets compiled and ran on each processor so use global context to understand what part of the data you need to process (depends on rank).

### Good Strategy 
- Cyclic round robin
- Start at `rank`, Then `i+=size`(num processors) until `i<n`.

## P2P communication
- Sender and receiver.
```c
if(rank==sender || rank == receiver){
	if (rank==sender) MPI_Send(...);
	else if (rank==receiver) MPI_Recv(); // Blocking
}
```
- Isolate using condition.
- Receiver must know sender.
- **Non-blocking**: only execute when message is received.
- **Synchronous**: Blocking plus handshake, tells it wants to send message before sending.