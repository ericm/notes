---
creation date: 2021-09-16 15:01
---
#  Grid Computing Lecture 1
16th September 2021

## Introduction
- 80/20 split.
- [Labs Website](http://www.cs.ucc.ie/~stabirca/teaching/CS4402/)  login: teaching/l_name

- Parallel tasks. 
- Use free space to critical path into smaller tasks.
- **Grand Challeng Problem**: Unfeasible
- **Supercomputer**: Parallel computer (lots of processors).

### Interconnect
- Processors have local memory
- Can share memory with wires or interconnect standard.

## MIMD & SIMD
- **SIMD**: Sequential Model
- **MIMD**: Parallel computing model: different instructions on different data.

## SPMD
- One Program: running on loads of different datasets on loads of cores.
- Has interconnect.
- NPI is SPMD.

## Speedup
- `(exec time one processor)/(exec time with n processors).`
- Delays with communication between processors.
- Num of OPs is theoritical execution time.
- n processors best speedup is n
### Superlinear Speedup
- If data is partitioned to fit in L1 cache, on a parallel scale, speedup may be greater than number of processors due to shared cache.
- Time to transfer via interconnects usually make speedup less than n.

## Amdalh's Law
### Given
- *n* processors.
- a sequential program that takes *T* seconds.
- *f%* cannot be done in parallel.

### Solution
- Take away *f%* that can't be done in parallel and put in grud.
- Slice rest of program into n parts.
- Scatter to processes
- **Speedup** = Tseq/Tpar = `T/(fT + (1-f)/n)T)` = `n/(f(n-1)+1)`

### Takeaways
- *f* decreases speedup.
- *n* increases speedup

## Gustafson's Law
### Given
- *n* processes
- a parallel program to run in Tseq
- *f%* is serial. For example, **initialising the data**.

### Solution
- Gives the speedup compared as sequential to parallel.
- **Speedup** = `n + (1-n)f`

### Explanation
- *n* increases speedup.
- *f* decreases speedup.
- `f = 0, S(n) = n`
- `f = 1, S(n) = 1`