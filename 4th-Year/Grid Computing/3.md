---
creation date: 2021-09-30 16:07
---
#  Grid Computing Lecture 3
30th September 2021

## P2P operations
- **Prefix**: `MPI_I`

## Collective
### Synchronization
- Reach point in program together.
### Data Movement
- Send out data and gather it.
### Computation
- Compute segments of data.
### Routines (+ Complexity)
- `MPI_BCast`: Send message to all other processes in that group.
- `MPI_Reduce`: Applies operation on data on processes in group and puts into buffer.
- `MPI_Scatter`: Scatter `sendcount` segments to processes in group.
	- Tstart + n/size Tcomm
- `MPI_Gather`
	- Tstart + n/size Tcomm
- sort
	- (n/size)^2 Tcomm
- merge
	- Sum from 1 to `size-1`.

## Pointers
- `malloc`: Memory alloc buffer of n bytes.
- `calloc`: Alloc x buffers of n bytes.

### Workflow
1. Scatter
2. Compute 
3. Reduce to global result.

## Parallel direct sort
- First processor node knows array with *n* elements.
- Scatter *n* elements of array to local array.
- Sort `n/size` elements of local array.
- Gather local array to array.
- Perform `size-1` merges on first processor.
```c
int MPI_Sort_direct(int n, int * array, MPI_Comm comm*) {
	MPI_Comm_size(comm,, &size);
	MPI_Comm_rank(comm &rank);
	int * local_array = (int*) calloc(n/size, sizeof(int));
	MPI_Scatter(array, n/size, MPI_INT,local_array, n/size, MPI_INT, root, comm);
	sort(n/size, local_array);
	MPI_Gather(local_array, n/size, MPI_INT, array, n/size, MPI_INT, root, comm);
	if (rank == root) { // Serial part in sequential program.
		for (i=n;i<size,i++) {
			merge(i*n/size, aray, n/size, array+i*n/size);
		}
	}
	return MPI_SUCCESS;
}
```