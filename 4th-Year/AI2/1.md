---
creation date: 2022-01-18 14:54
---
#  AI2 Lecture 1
18th January 2022

## Text
- Convert to vectors.

## Background
- **U**: Universal set: all possible objects.
- Describe a set by a binary valued vector.

## Bags
- A set with duplicates allowed.

## Bag-of-Words.
- Bag of words for each document.
### Tokenization
- Putting words into a vector without punctuation. 
- Normalizing.
- Take into account cases where punctuation concatonate words, etc.
#### Bigrams
- Additional sliding window pairs of consecutive words.
### Stop-words
- Common words that do not discriminate, can be removed in most cases.
### Stemming
- Removing -ing, etc
### Lemmatization
- Turns gave into give.
- Definitive form conversion.

- One word per column in a vector. - features.
- Fill features with counts of words, count vector.

## Singular Value Decomposition
- Like Principle Component Analysis.