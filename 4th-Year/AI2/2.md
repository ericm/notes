---
creation date: 2022-01-20 11:05
---
#  AI2 Lecture 2
20th January 2022

## Natural Language Processing
- Treat documents as sequences: lists: maintain order.
- Give each word a unique number during preprocessing. 
	- May include punctuation.
- Feed in batches of sentences simultaneously.
- Pad smaller documents in batch with zeros.
- Truncating longer documents to conform to a uniform length may be required.
### Index
- Indexes may mislead algorithm due to similarity in indexes not representing similarity in meaning.
- Use One-Hot Encoding.
### Word Embedding
- One Hot encoding gives 0 similarity/crossover between words, not good at finding joined meaning between words.
- **Word Embeddings** are small, non-sparse vectors.
- Encoded using a fixed size vector, making words with similar meanings having values closer to each other.
- Reflects semantic relationships between words.
- Relationships between types of words should be represented by a static transformation of values. Eg Dog to Wolf should take the same values as Cat to Tiger.
- Reflecting on sentences (documents) with a slight change can be used to intuit how likely the meaning of a sentence is or what word should come next.
#### Learning Word Embeddings
1. Initialize randomly.
2. Feed in sentences.
3. Embedding layer processes based on embedding dimension (number of values representing each word).
4. Finds relationships by processing loads of documents and finding similarities.
