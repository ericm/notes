---
creation date: 2022-01-25 15:02
---
#  AI2 Lecture 3
25th January 2022

##  Recurrent Neural Networks
- Not Feed Forward.
- Better for Sequence Data.
- All words were fed in one go before with Word Embeddings.
- Feed in one word at a time. Recollection of what previous was. - Stateful RNN.
- `SimpleRNN`.
- Feeding words one at a time to Neural Network
- Feed in state of NN from previous iteration.
- Output is activation(xi\*wi + prev_output_i\*prev_weight_i + bias).
- Often only take final output.
- Shape of input is number of weights in state of output + shape of input word (if onehot encoded it will be number of words) + 1 (for bias).
### Training
- RETURN SEQUENCES = TRUE = Return all output 
	- m = num sequences, maxlen = max words.  n = vocab size.
	- IN (m, maxlen, n)
	- OUT (m, maxlen, 1)
- RETURN SEQUENCES = False = Return all output 
	- IN (m, maxlen, n)
	- OUT (m, 1)
- Feeding history into next layer - RETURN SEQ TRUE.
- Back propogation through time - Judge how much weights from previous stage  contribute to errors.
	1. Random init
	2. Forward Prop
	3. Backprop on unrolled network
	4. Update all weights
- For SimpleRNN layer 
	- ((units + size of prev output) x units) + units (biases)
- Dense Layer = (size x prev output) + size
- Embed layer = vocab size x embedding size.


## Long Short-Term Memory Neurons
- Outputs same output and **carry**, used to intuit more than one step behind.
## Overfitting
