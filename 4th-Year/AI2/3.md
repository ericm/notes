---
creation date: 2022-01-25 15:02
---
#  AI2 Lecture 3
25th January 2022

##  Recurrent Neural Networks
- `SimpleRNN`.
- Feeding words one at a time to Neural Network
- Feed in state of NN from previous iteration.
- Output is activation(xi\*wi + prev_output_i\*prev_weight_i + bias).
- Often only take final output.
- Shape of input is number of weights in state of output + shape of input word (if onehot encoded it will be number of words) + 1 (for bias).
### Training
- Back propogation through time - Judge how much weights from previous stage  contribute to errors.
- For SimpleRNN layer with units=256 , the total number of parameters is **256 + 256 × 256 + 256 × 28 = 72,960** corresponding to bias, weight, and units (shape of output) = 28 contributions.

## Long Short-Term Memory Neurons
- Outputs same output and **carry**, used to intuit more than one step behind.