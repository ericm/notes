---
creation date: 2021-10-06 17:10
---
#  AI1 Lecture 8
6th October 2021

## K-Nearest Neighbours
- Uses SGD.
- Needs Scaling

## Linear Equation
- Uses MSE.
- No scaling needed.

## Learning
- **Mean Absolute Error**: average difference between predicted and actual values

## Holdout
Split data into training and testing.
- **Cheating**: is called leakage. (**Overfitting**).
- Calculate mean and SD on only training data set.
- Scale based on mean and SD of training data.
	
## K-Fold Cross-Validation
Better for smaller data sets.
1. Shuffle Data.
2. Split data into k sets.
3. Train on everything except partition i.
4. Test on partition i.
5. Increment i and repeat.
6. Average loss at end is loss.