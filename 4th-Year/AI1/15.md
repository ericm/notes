---
creation date: 2021-11-03 17:11
---
#  AI1 Lecture 15
3rd November 2021

## Decision Tree
- Binary Tree.
- X hand side of node should mean a particular class.
- Break down dataset until y class values are *pure*.
	- Has 0 and another value.
	- This means they only contain one desired class.
	- *gini* is score of how pure they are.
- If gini is not 0, split it.
	- Max depth will stop split.
- May not end up pure if there is duplicate values with different classes.
- Can be used for numeric values as well.
	- Right hand side greater, left hand side less.
- Goes through a list of questions (conditions on different features).

## Parametric Learning
- Number of examples known.
### Examples
- Linear: n+1 params

## Non-Parametric Learning
- Number of params not known.

## Ensembles
- Like a committee for models.
- Multiple models together.
- `VotingClassifier`
### Bagging
- Lots of decision trees.
	- Random Forrest.
- Get concensus from most outputed decision from trees.