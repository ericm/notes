---
creation date: 2021-11-24 17:04
---
#  AI1 Lecture 21
24th November 2021

## Convolutional Neural Networks
- Higher layers combine information from lower layers to understand a greater picture.
- Not like dense layers, everything is not connected to everything.
### Images
- Don't flatten, keep images as 3 Tensors.
- Flattening loses contiguity (things being next to each other).
- Output dense layer should have a hidden layer for each class.

## Layers
### Conv2D
- Neuron connected to an nxn window of neurons in the layer below.
- Window is shifted along (stride (default 1)) on each subsequent neuron.
- Layers have reduced neurons if their window goes off the edge of the previous layer.
	- **Feature map**.
	- h-4 x w-4 on second layer.
	- Padding will pad out layers to allow for non reduced upper layer sizes.
- Weights are shared between neuron links (w weights as opposed to w x (h-4 x w-4) weights.).
	- Translational invariance.
#### Stacking
- Feature map can be connected to multiple layers below (in a dense layer or x tensor.)
- Gives us overview of whole dense layer.
### MaxPooling
- nxn - next to each other, dont overlap - pooling layer is halt the width and haplf the height.
- Gets nxn window and gets max - halves the number of pieces the data.