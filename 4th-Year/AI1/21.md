---
creation date: 2021-11-24 17:04
---
#  AI1 Lecture 21
24th November 2021

## Convolutional Neural Networks
- Higher layers combine information from lower layers to understand a greater picture.
- Not like dense layers, everything is not connected to everything.
### Images
- Don't flatten, keep images as 3 Tensors.
- Flattening loses contiguity (things being next to each other).
- Output dense layer should have a hidden layer for each class.

## Layers
### Conv2D
- Neuron connected to an nxn window of neurons in the layer below.
- Window is shifted along (stride (default 1)) on each subsequent layer.
- Layers have reduced neurons if their window goes off the edge of the previous layer.
	- **Feature map**. ^96dc28
	- h-4 x w-4 on second layer.
	- Padding will pad out layers to allow for non reduced upper layer sizes.
- Weights are shared between neuron links (w weights as opposed to w x (h-4 x w-4) weights.).
	- Translational invariance.
- Stack Feature Maps between 
### MaxPooling